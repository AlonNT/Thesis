# Beyond Gradient-Descent
Trying to figure out a way to optimize neural-networks differently, 
that will possibly allow parallelization over the layers 
(inherently impossible in back-propagation).