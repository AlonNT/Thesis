# Beyond Back-Propagation
Trying to figure out a way to optimize neural-networks differently, 
that will possibly allow parallelization over the layers 
(inherently impossible in back-propagation).

# Related Work

## Representation Learning

### [A Simple Framework for Contrastive Learning of Visual Representations]

This paper presents SimCLR: a simple framework for contrastive learning of visual representations.

### [Big Self-Supervised Models are Strong Semi-Supervised Learners]

Future read.

### [Representation Learning with Contrastive Predictive Coding]

Future read.

### [Context Encoders: Feature Learning by Inpainting]

Future read.

### [Predicting What You Already Know Helps: Provable Self-Supervised Learning]

Future read.

### [Putting An End to End-to-End: Gradient-Isolated Learning of Representations]

### [LoCo: Local Contrastive Representation Learning]

## Synthetic Gradients

### [Decoupled Neural Interfaces using Synthetic Gradients]

### [Understanding Synthetic Gradients and Decoupled Neural Interfaces]

## Layerwise Optimization

### [A Provably Correct Algorithm for Deep Learning that Actually Works]

### [Greedy Layerwise Learning Can Scale to ImageNet]

### [Decoupled Greedy Learning of CNNs]

### [Parallel Training of Deep Networks with Local Updates]

## Feedback Alignment

### [Random feedback weights support learning in deep neural networks]
Nature communications version: [Random synaptic feedback weights support error backpropagation for deep learning]

### [Direct Feedback Alignment Provides Learning in Deep Neural Networks]

### [Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures]

## Books

### [Convex Optimization]
### [Online Learning and Online Convex Optimization]


[A Simple Framework for Contrastive Learning of Visual Representations]: https://arxiv.org/pdf/2002.05709.pdf
[Big Self-Supervised Models are Strong Semi-Supervised Learners]: https://arxiv.org/pdf/2006.10029.pdf
[Representation Learning with Contrastive Predictive Coding]: https://arxiv.org/pdf/1807.03748.pdf
[Context Encoders: Feature Learning by Inpainting]: https://arxiv.org/pdf/1604.07379.pdf
[Predicting What You Already Know Helps: Provable Self-Supervised Learning]: https://arxiv.org/pdf/2008.01064.pdf
[Putting An End to End-to-End: Gradient-Isolated Learning of Representations]: https://arxiv.org/pdf/1905.11786.pdf
[LoCo: Local Contrastive Representation Learning]: https://arxiv.org/pdf/2008.01342.pdf
[Decoupled Neural Interfaces using Synthetic Gradients]: https://arxiv.org/pdf/1608.05343.pdf
[Understanding Synthetic Gradients and Decoupled Neural Interfaces]: https://arxiv.org/pdf/1703.00522.pdf
[A Provably Correct Algorithm for Deep Learning that Actually Works]: https://arxiv.org/pdf/1803.09522.pdf
[Greedy Layerwise Learning Can Scale to ImageNet]: https://arxiv.org/pdf/1812.11446.pdf
[Decoupled Greedy Learning of CNNs]: https://arxiv.org/pdf/1901.08164.pdf
[Parallel Training of Deep Networks with Local Updates]: https://arxiv.org/pdf/2012.03837.pdf
[Random feedback weights support learning in deep neural networks]: https://arxiv.org/pdf/1411.0247.pdf
[Random synaptic feedback weights support error backpropagation for deep learning]: https://www.nature.com/articles/ncomms13276.pdf
[Direct Feedback Alignment Provides Learning in Deep Neural Networks]: https://arxiv.org/pdf/1609.01596.pdf
[Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures]: https://arxiv.org/pdf/2006.12878.pdf
[Convex Optimization]: https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf
[Online Learning and Online Convex Optimization]: https://www.cs.huji.ac.il/~shais/papers/OLsurvey.pdf